{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1950a0a-e6e8-430e-88e7-052c654e9aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\boss\\appdata\\roaming\\python\\python312\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\boss\\appdata\\roaming\\python\\python312\\site-packages (from sentence_transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\boss\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.0 MB 2.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/10.0 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.6/2.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed safetensors-0.4.5 sentence_transformers-3.3.1 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\BOSS\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aada61f-7c67-4f86-838b-a11c8c272280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.8 MB 1.5 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/13.8 MB 2.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.8/13.8 MB 3.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.4/13.8 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.3/13.8 MB 5.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.6/13.8 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.1/13.8 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.2/13.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.5/13.8 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.3/13.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.8/13.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.8 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f859e84-9aa3-4085-961b-13caf6a5cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "\n",
    "# Load SentenceTransformer for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = \" \".join([page.extract_text() for page in pdf.pages])\n",
    "    return text\n",
    "\n",
    "# Indexing function\n",
    "def index_resume_content(resume_text):\n",
    "    sentences = resume_text.split(\"\\n\")\n",
    "    embeddings = model.encode(sentences)\n",
    "    dimension = embeddings.shape[1]\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, sentences\n",
    "\n",
    "# Load and index resume content\n",
    "resume_text = extract_text_from_pdf(\"saqlain.pdf\")\n",
    "resume_index, resume_sentences = index_resume_content(resume_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c5fd8c-756a-457d-a92f-9a8b70c539aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muhammad Saqlain\n",
      "Machine Learning Engineer\n",
      "Gujranwala,Pakistan•+92-3127430812•saqlainnadeem812@gmail.com•linkedin.com/in/msaqlain786\n",
      "As adedicatedMachineLearningEngineer,graduatedfromFastNUCES,Ispecializeindesigning,implementing,and\n",
      "optimizing machine learning models to drive data-driven solutions. With asolidbackgroundinPython,TensorFlow,\n",
      "andscikit-learn.IamproficientinusingcloudplatformsfordevelopinganddeployingAIandMLmodels.\n",
      "RELEVANT WORK EXPERIENCE\n",
      "BracketsPvtLtd,Pakistan,Gujranwala 2024–Present\n",
      "AssociateSoftwareEngineer\n",
      "● DevelopedMERNstackapplicationsandmicroserviceswithNodeJS,ExpressJS,NestJS,integrating\n",
      "RESTfulAPIsforpaymentsandcommunication.UtilizedAWSEC2forcompute,S3forstorage,SES\n",
      "foremail,SNSformessaging,andAuth0forauthentication.\n",
      "● EngineeredscalablebackendsolutionswithNode.jsandNestJS,interfacingwithMongoDBfor\n",
      "efficientdatahandling.UtilizedCodestandardformodularityandcodequality.\n",
      "● OptimizeddatastorageandperformanceinMongoDBforhigh-trafficapplications,andintegratedenterprise\n",
      "APIswithAWScloudservices.UsedJWT,andAuth0forcomprehensiveusermanagementandsecurity.\n",
      "● UtilizedGenAIforcreativityandtimemanagementresultingin60%reductionindevelopmenttimeand\n",
      "reduce80%bugs\n",
      "SoftechSystems,Pakistan,Lahore 2023–2024\n",
      "JavaSoftwareEngineer\n",
      "● DesignedandimplementedscalableenterpriseapplicationsusingSpringBoot,OracleDBandHibernate.\n",
      "CreatedRESTfulAPIs,improvingapplicationperformanceandreducingresponsetimesby40%.Managed\n",
      "developmentusingAgilemethodologiestoensureiterativeimprovementsandtimelydelivery.\n",
      "● RefactoredlegacyStrutsandJSPsystemswithmodernSpringfeatures,boostinguser\n",
      "satisfactionby25%andcuttingpageloadtimesby20%.\n",
      "● Optimizeddataaccesslayers foranInvestmentManagementplatform,enhancingefficiencyandreducing\n",
      "databasetransactiontimeby35%.\n",
      "● UtilizedObjectOrientedPrinciples,DesignPatterns,SoftwareArchitectureandsecuritybest\n",
      "practicestoimprovecodequality,maintainabilityandtestability\n",
      "FreelanceDataScientist,Upwork 2024–Pre\n",
      "DataScientist\n",
      "● Developedanddeployedarobustcustomerchurnpredictionmodelusingadvancedmachinelearning\n",
      "techniquesandextensivedatasetanalysis.\n",
      "● Accuratelyidentifiedat-riskcustomers,enablingretentionstrategiesandreducingchurnratesby30%.\n",
      "● Providedactionableinsightsthatempoweredclientstotailortheirengagementstrategies,leadingto\n",
      "improvedcustomersatisfactionandincreasedretention.\n",
      "● Scraped10+insurancewebsitesdatausingPuppeteer(JavaScript)andScrapy(Python)\n",
      "● Analyzeddatausingpythonforinsightsandrecommendationtoimprovedclaimprocessingtime\n",
      "● Developedaclusteringmodelforanalyzingcustomerdemographicsandtargeting.\n",
      "EDUCATION\n",
      "FastNationalUniversityofComputerandEmergingScience,Islamabad,Pakistan 2023\n",
      "BachelorofScience—SoftwareEngineering\n",
      "SKILLS\n",
      "TechnicalSkills:Python,Java,JavaScript,Git,Docker,BusinessIntelligenceandAnalytics(Advance),DeepLearning\n",
      "(Basic),PredictiveModeling(Intermediate),ComputationalIntelligence,NLP, AWS,StatisticalAnalysis,Algorithms\n",
      "Certifications:AdvanceBusinessAnalytics:Coursera\n",
      "Other:AgileProjectManagement,ProcessImprovement,English(Professional),Urdu(Native)\n"
     ]
    }
   ],
   "source": [
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0552d64-82f6-492e-99bc-25e2bdbd31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000013FE8A9FD80> >\n"
     ]
    }
   ],
   "source": [
    "print(resume_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d001b127-8997-47cf-880c-cff74c28afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def retrieve_context(query, index, sentences, top_k=6):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    return [sentences[i] for i in indices[0]]\n",
    "\n",
    "def get_ai_response(user_input):\n",
    "    # Retrieve context from the resume\n",
    "    context = retrieve_context(user_input, resume_index, resume_sentences)\n",
    "\n",
    "    # Combine user input with retrieved context\n",
    "    combined_prompt = f\"Context: {' '.join(context)}\\nQuestion: {user_input}\\nAnswer:\"\n",
    "\n",
    "    # Prepare payload for the API\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": combined_prompt,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    url = \"https://llm.kryptomind.net/api/generate\"\n",
    "\n",
    "    headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        ai_response = \"\"\n",
    "        try:\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = line.decode('utf-8')\n",
    "                    chunk_json = json.loads(chunk)\n",
    "                    if 'response' in chunk_json:\n",
    "                        ai_response += chunk_json['response']\n",
    "\n",
    "            # Store the question and answer in MongoDB\n",
    "            qa_pair = {\n",
    "                \"question\": user_input,\n",
    "                \"context\": context,\n",
    "                \"answer\": ai_response\n",
    "            }\n",
    "            # collection.insert_one(qa_pair)\n",
    "\n",
    "            return ai_response\n",
    "\n",
    "        except ValueError as e:\n",
    "            return f\"Error parsing JSON: {e}\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cd2f3bb-9ab7-435b-9fe0-f43521ced8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems you're starting a new conversation about your resume! As a personal bot, I'm here to help. You haven't specifically asked any questions yet, but that's okay!\n",
      "\n",
      "Let me summarize the information provided so far:\n",
      "\n",
      "**EDUCATION SKILLS**\n",
      "\n",
      "* Machine Learning Engineer at BracketsPvtLtd (Pakistan) from 2024-present\n",
      "\t+ Phone number: +92-3127430812\n",
      "\t+ Email: saqlainnadeem812@gmail.com\n",
      "\t+ LinkedIn profile: linkedin.com/in/msaqlain786\n",
      "\n",
      "**WORK EXPERIENCE**\n",
      "\n",
      "* No specific experience mentioned yet, but we can discuss this further!\n",
      "\n",
      "Feel free to ask me any questions about your resume, and I'll be happy to help you with answers. What would you like to know or focus on next?\n"
     ]
    }
   ],
   "source": [
    "print(get_ai_response(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38b131e0-0888-4093-b594-6ac38209883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://5fe01e76feb3adb428.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5fe01e76feb3adb428.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "iface = gr.Interface(fn=get_ai_response, \n",
    "                     inputs=\"text\",\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Resume RAG QA Bot\", \n",
    "                     description=\"Ask questions based on the resume PDF.\")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec2488-aa1b-4fca-97f1-01efe61104aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
